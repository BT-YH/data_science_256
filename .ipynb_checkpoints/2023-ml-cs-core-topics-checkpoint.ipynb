{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2023 AI/Machine Learning Topics (CS Core)](https://csed.acm.org/wp-content/uploads/2023/03/Version-Beta-v2.pdf)\n",
    "\n",
    "## Definition and Examples of a Broad Variety of Machine Learning (ML) tasks\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) is essentially [function approximation](https://en.wikipedia.org/wiki/Function_approximation) or [curve fitting](https://en.wikipedia.org/wiki/Curve_fitting) that is generalized to many [data types](https://en.wikipedia.org/wiki/Statistical_data_type) (e.g. [nominal, ordinal, discrete, continuous](https://en.wikipedia.org/wiki/Statistical_data_type#Simple_data_types)) of possibly many dimensions.  Given a set of pairs of inputs with associated outputs (i.e. \"feature vectors labeled with labels\"), compute a function (i.e. \"learn a model\") that minimizes expected output prediction error according to a loss function.\n",
    "\n",
    "#### Classification\n",
    "\n",
    "[Classification](https://en.wikipedia.org/wiki/Statistical_classification) is supervised learning for discrete outputs.\n",
    "\n",
    "Examples:\n",
    "* [Spam filtering](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering): Given a training set of message [word frequencies](https://en.wikipedia.org/wiki/Bag-of-words_model) labeled \"spam\"/\"not spam\", learn to detect (predict) spam from unseen message word frequencies.\n",
    "* [Optical character recognition](https://en.wikipedia.org/wiki/Optical_character_recognition#Text_recognition): Given a training set of character images labeled with characters represented, learn to recognize (predict) which character is represented from unseen character images.  A common example is the benchmark [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) of handwritten digit characters.\n",
    "* [Facial recognition](https://en.wikipedia.org/wiki/Facial_recognition_system): In the last stage of a multistage [deep learning face recognition system](https://arxiv.org/abs/1804.06655), given a set of face representations labeled with IDs, learn to associate unseen, varied instances of face representation to the correct IDs.\n",
    "\n",
    "#### Regression\n",
    "\n",
    "[Regression](https://en.wikipedia.org/wiki/Regression_analysis) is supervised learning for continuous outputs.\n",
    "\n",
    "Examples:\n",
    "* [Housing Price Prediction](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) (see also [Boston](https://scikit-learn.org/1.0/modules/generated/sklearn.datasets.load_boston.html) and [California](https://scikit-learn.org/1.0/modules/generated/sklearn.datasets.fetch_california_housing.html) datasets): Given a set of house feature vectors (including, for instance, square footage, acreage, number of bedrooms, etc.) labeled with selling prices, predict the selling price of a house given its feature vectors.\n",
    "* TODO - more examples\n",
    "\n",
    "### Reinforcement Learning (RL)\n",
    "\n",
    "[Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning) algorithms learn how to map *states* to *actions* that yield *reward* and *next state* probabilities so as to _maximize expected cumulative future rewards_.  Problems are usually models as [Markov Decision Processes (MDPs)](https://en.wikipedia.org/wiki/Markov_decision_process), so algorithms often make use of [Bellman's optimality equations](https://en.wikipedia.org/wiki/Bellman_equation).\n",
    "\n",
    "Introductory textbook: Sutton, Richard S., Barto, Andrew G.  [Reinforcement Learning: An Introduction. 2nd ed.](http://incompleteideas.net/book/the-book-2nd.html), 2020. ([PDF](http://incompleteideas.net/book/RLbook2020.pdf))\n",
    "\n",
    "* TODO - examples\n",
    "\n",
    "### Unsupervised learning\n",
    "\n",
    "[Unsupervised Learning](https://en.wikipedia.org/wiki/Unsupervised_learning), in contrast to supervised learning, concerns algorithms for learning patterns in _unlabeled_ data, i.e. input feature vectors with _no_ outputs.  Example problems include [clustering](https://en.wikipedia.org/wiki/Cluster_analysis) (e.g. [k-Means Clustering](http://modelai.gettysburg.edu/2016/kmeans/)) and [anomaly detection](https://en.wikipedia.org/wiki/Anomaly_detection).\n",
    "\n",
    "* [Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) is grouping a set of objects such that objects in the same group (i.e. cluster) are more similar to each other in some sense than to objects of different groups.  \n",
    "  * In the context of [k-Means Clustering](http://modelai.gettysburg.edu/2016/kmeans/), objects are real-valued vectors, more similar objects have lesser Euclidean distance, and we seek to compute and assignment of each vector to one of $k$ cluster groups such that the sum of squared distances from each point to its cluster _centroid_ (mean cluster point) is minimized, i.e. a minimal within-cluster sum-of-squares (WCSS). The [archived OnMyPhd website](https://web.archive.org/web/20210614173002/http://www.onmyphd.com/?p=k-means.clustering) describes and visualizes the k-Means Clustering algorithm that chooses initial centroids and then iteratively, alternately remaps points to their closest centroids and recomputes centroids based on the current cluster assignments until such iteration converges to a fixed local (but not necessarily global) WCCS minimum\n",
    "  * In this [Scikit-Learn comparison of different clustering algorithms on \"toy\" datasets](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html) 2D colored scatter plots illustrate strengths and weaknesses of different algorithms for different datasets.  \n",
    "\n",
    "* TODO - more examples\n",
    "\n",
    "## Fundamental ML ideas:\n",
    "\n",
    "* *The NFL (No Free Lunch) Theorem of Machine Learning* - Wolpert and Macready summarize their [\"No Free Lunch Theorem\"](https://en.wikipedia.org/wiki/No_free_lunch_theorem) as stating that \"any two optimization algorithms are equivalent when their performance is averaged across all possible problems\".  This is not to say that practical problems so not exhibit structures, regularity, or tendency towards simplicity ([Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor)) for which optimization algorithms (such as ML algorithms) do not have performance advantages.  Rather, that without nonuniform prior probabilities over target functions to be approximated, we have no reason to believe any one learning algorithms is \"better\" than another.  See the [\"Implications\"](https://en.wikipedia.org/wiki/No_free_lunch_theorem#Implications) section of the Wikipedia article to consider the NFL theorem implications in the context of [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). \n",
    "\n",
    "* *Undecidability of ML* - In Ben-David, S., Hrubeš, P., Moran, S. et al. [Learnability can be undecidable](https://doi.org/10.1038/s42256-018-0002-3). Nat Mach Intell 1, 44–48 (2019), the authors constructed an \"Estimating the Maximum Problem\" (EMX) learning model, and showed that a family of problems functions whose learnability in EMX is undecidable in standard set theory.  They related learnability to a compression problem which in turn is related to a question on the cardinality of a set, i.e. whether or not it is uncountable.  Given the connection between learnability, compression, and cardinality, and given that many statements regarding cardinalities are undecidable, learnability in some cases is undecidable.  An [undecidable problem](https://en.wikipedia.org/wiki/Undecidable_problem) is one for which it can be proven that no algorithm exists that can provide a correct \"yes\"-or-\"no\" answer.  \n",
    "\n",
    "* *Sources of error in ML* - Three primary sources of error in ML are:\n",
    "** *Noise* (a.k.a. \"irreducible error\", error variance in [Introduction to Statistical Learning, Chapter 2](https://www.statlearning.com/)) in data which cannot be be addressed through better choice of model or learning algorithm.\n",
    "** *Bias* from the choice of a ML model that is simpler than the true model underlying the data, e.g. error from a linear regression of nonlinear data.\n",
    "** *Variance* from the way an ML model may vary in what is learned from one dataset to another, even from the same underlying source of data.\n",
    "\n",
    "To summarize, our best efforts to learn best ML models must necessarily make prior assumptions of a nonuniform probability over possible functions, can suffer from undecidability, and, even when we have minimized bias and variance error through the choice of most appropriate models, may still have irreducible error in the form of noise.  Nevertheless, while theoretical results and sources of error may challenge and limit, successful applications of ML provide evidence of the practical benefits of ML techniques.  Worst case assumptions, analyses, and proofs can temper but not extinguish the optimism brought by the impressive artifacts of this discipline.  \n",
    "\n",
    "\n",
    "## Simple statistical-based supervised learning such as Naive Bayes, Decision trees\n",
    "(Focus on how they work without going into mathematical or optimization details; enough to understand and use existing implementations correctly)\n",
    "\n",
    "### Naive Bayes\n",
    "[Naive Bayes classification](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) is one of the simplest classification methods in use.  Based on [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) and an assumption of [conditional independence](https://en.wikipedia.org/wiki/Conditional_independence) between every pair of input features.  For each possible class output, we multiply the probability of that class output times each of the probabilities of having the given input feature with that output.  We then choose the class for which this product is maximized for our classification. \n",
    "\n",
    "Example Python use of Naive Bayes classification:\n",
    "* [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "* [Datacamp tutorial](https://www.datacamp.com/tutorial/naive-bayes-scikit-learn)\n",
    "\n",
    "### Decision Trees\n",
    "In the Number Guessing Game, one person chooses a secret integer in a range (e.g. 1 - 100) and the other player seeks to guess the secret number in as few guesses as possible, with each guess followed by the feedback \"higher\", \"lower\", or \"correct\".  A good player of the game starts with a guess in the middle of possibilities resulting in either a correct guess (rarely), or a reduction in the possible range by half.  In contrast, a poor player could guess the numbers in order (\"1\", \"2\", etc.).  What makes a middle guess good is that it maximizes information gained by evenly splitting possibilities.\n",
    "\n",
    "Similarly, in supervised learning, we may construct a [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning) by asking a question of a single input feature at the root node, dividing cases down possible answer branches, and repeating this process at subsequent child nodes.  Each tree node question is generally selected to best gain information about probable outputs.  Stopping conditions for the recursive growth of the decision tree may vary, but each leaf node of the tree can use its relevant subset of input/output cases to predict an output, either through classification (e.g. most popular of class outputs) or regression (e.g. linear regression on node cases).\n",
    "\n",
    "Example Python use of Decision Trees:\n",
    "* [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/tree.html)\n",
    "* [Datacamp tutorial](https://www.datacamp.com/tutorial/decision-tree-classification-python)\n",
    "* [Kaggle tutorial](https://www.kaggle.com/code/dansbecker/your-first-machine-learning-model)\n",
    "\n",
    "## The overfitting problem and controlling solution complexity (regularization, pruning – intuition only)\n",
    "### The bias (underfitting) - variance (overfitting) tradeoff\n",
    "\n",
    "The [Bias-Variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) concerns the tension between reducing model choice bias, i.e. underfitting the function due to choice of overly simplistic model functions, and avoiding variance, i.e. overfitting the model to the noise (i.e. \"irreducible error\") by choice of overly complex model function.  An example of too much bias would be a linear regression of data generated from a cubic function with noise.  An example of too much variance would be a very high-degree polynomial regression that seeks a tight fit to the same noisy data.  \n",
    "\n",
    "Examples:\n",
    "* [Scipy Lecture Notes](https://scipy-lectures.org/packages/scikit-learn/auto_examples/plot_bias_variance.html)\n",
    "* [Aqeel Anwar Python polynomial illustration](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-and-visualizing-it-with-example-and-python-code-7af2681a10a7)\n",
    "\n",
    "[Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) is a technique for introducing a bias towards learning simpler models by penalizing more complex models via the loss function we seek to minimize when learning the model.  For example, one might introduce penalties for having more non-zero model parameters, keeping models simple and sparse, or for having large model parameters that would make the learned function less smooth.  [Early stopping](https://en.wikipedia.org/wiki/Early_stopping) in learning by terminating model learning before the model overfits (or after there are indications of overfitting and reversion to an earlier iteration model) can be viewed as a form of regularization in time.\n",
    "\n",
    "[Pruning in neural networks](https://en.wikipedia.org/wiki/Pruning_(artificial_neural_network)) involved the removal of edges (weights) and nodes (neurons) that are least important by some measure in order to move towards a more efficient, less overfitting model.  Decision tree growth stopping conditions (e.g. maximum depth limits, node purity thresholds, etc.) are another means for limiting variance from overfitting.\n",
    "\n",
    "\n",
    "## Working with Data\n",
    "### Data preprocessing\n",
    "#### Importance and pitfalls of\n",
    "### Handling missing values (imputing, flag-as-missing)\n",
    "#### Implications of imputing vs flag-as-missing\n",
    "### Encoding categorical variables, encoding real-valued data\n",
    "### Normalization/standardization\n",
    "### Emphasis on real data, not textbook examples\n",
    "\n",
    "## Representations\n",
    "### Hypothesis spaces and complexity\n",
    "### Simple basis feature expansion, such as squaring univariate features\n",
    "### Learned feature representations\n",
    "\n",
    "## Machine learning evaluation\n",
    "### Separation of train, validation, and test sets\n",
    "### Performance metrics for classifiers\n",
    "### Estimation of test performance on held-out data\n",
    "### Tuning the parameters of a machine learning model with a validation set\n",
    "### Importance of understanding what your model is actually doing, where its pitfalls/shortcomings are, and the implications of its decisions\n",
    "\n",
    "## Basic neural networks\n",
    "### Fundamentals of understanding how neural networks work and their training process, without details of the calculations\n",
    "\n",
    "## Ethics for Machine Learning\n",
    "### Focus on real data, real scenarios, and case studies.\n",
    "### Dataset/algorithmic/evaluation bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
